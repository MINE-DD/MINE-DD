{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab959852-e492-42fc-af79-11a45ccf5ace",
   "metadata": {},
   "source": [
    "# AskPoopQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467e843-eb28-42dc-88fc-adff47e6f475",
   "metadata": {},
   "source": [
    "This notebook builds a langgraph that showcases the potential of a RAG agent that summarises scientific evidence from the literature and extracts statistical significant associations between pathogens and enviromental factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a261e50-c86a-4c8d-9748-db883a4ed771",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23f28e0e-d670-4678-8f36-053bffe9b9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from os.path import exists\n",
    "import sqlite3\n",
    "# LLM\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import Document, AIMessage\n",
    "# to chunk the text\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# to make/store embeddings \n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "#from langchain_community.embeddings.spacy_embeddings import SpacyEmbeddings\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "# to build/display/run a langgraph\n",
    "from langgraph.graph import StateGraph\n",
    "from IPython.display import Image, display\n",
    "import operator\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List, Annotated\n",
    "from langgraph.graph import END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076fada-99f5-40f2-9fa7-6c5a23e27835",
   "metadata": {},
   "source": [
    "#### Set-up the LLM\n",
    "\n",
    "Do you have ollama running?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3603ea-8c3f-47db-afb9-b5c9c0e45eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = \"llama3.1:70b\"\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "llm_json_mode = ChatOllama(model=local_llm, temperature=0, format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f28008-99e0-4429-b4fa-0f67ef243428",
   "metadata": {},
   "source": [
    "## Note: Skip to the embeddings part if you don't have the `db` file as the embeddings are run on the text contained in the database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254da7c5-731a-4f26-be2a-e7ebe62ea3f1",
   "metadata": {},
   "source": [
    "#### Load database `db` of papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c77352d-51f2-4707-bea9-33ee88e28df1",
   "metadata": {},
   "source": [
    "This is the path of the `db` database containing the text extracted from the relevant papers with llamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9ecec23-da9b-4d14-80bb-b85136e8c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = '/projects/0/prjs1194/literature_relevant.db'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea33170-e7b0-43eb-b04c-94c050b15dce",
   "metadata": {},
   "source": [
    "Define `extract_full_text_content` - this function below takes as input the SQL database and returns the full-text of the papers per page content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f3b28d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_full_text_content(database_path):\n",
    "    conn = sqlite3.connect(database_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Retrieve all rows/papers from the table\n",
    "    cursor.execute(f\"SELECT fulltext, DOI FROM literature_fulltext;\")\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    documents = []\n",
    "    # Add paper text and dict of metadata\n",
    "    for row in rows:\n",
    "        fulltext, doi = row\n",
    "        if isinstance(fulltext, str) and fulltext is not None:\n",
    "            documents.append({\n",
    "                \"content\": fulltext,\n",
    "                \"metadata\": {\n",
    "                    \"doi\": doi\n",
    "                }\n",
    "            })\n",
    "\n",
    "    conn.close()\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0debe0fe-abb9-4fab-9a52-c86be7164ce7",
   "metadata": {},
   "source": [
    "Extract the content below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe0815fb-8e72-4e05-89b9-1b4aa5b32544",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = extract_full_text_content(database_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9972b1b8-6c88-4a9d-9de1-68d834eeb2e2",
   "metadata": {},
   "source": [
    "`docs` is a list containing the full text per paper. This is not directly usable for `langchain` and it must be converted into a `Document` istance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c41d1-b1fd-4212-b382-a28654a540db",
   "metadata": {},
   "source": [
    "We do this together with the splitting, by using the function `.create_documents` while splitting it, within the `RecursiveCharacterTextSplitter`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf10e28-d900-4183-9b40-1ce59dfd9756",
   "metadata": {},
   "source": [
    "#### Chunk text and extract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ca33d72-2295-497c-8068-2e6fcd28f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=2000, chunk_overlap=200\n",
    ")\n",
    "\n",
    "documents = text_splitter.create_documents([doc[\"content\"] for doc in docs], \n",
    "                                           metadatas=[doc[\"metadata\"] for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b796b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': 'paper 10.1371/journal.pone.0071616 44'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check first document\n",
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "820f25ed-57cf-4e7b-85d3-526685e6fde5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n",
      "# Rotavirus Seasonality and Age Effects in a Birth Cohort Study of Southern India\n",
      "\n",
      "#\n",
      "# Rotavirus Seasonality and Age Effects in a Birth Cohort Study of Southern India\n",
      "\n",
      "Rajiv Sarkar1, Gagandeep Kang1, Elena N. Naumova1,2*\n",
      "\n",
      "1Department of Gastrointestinal Sciences, Christian Medical College, Vellore, TN, India\n",
      "\n",
      "2Department of Civil and Environmental Engineering, Tufts University School of Engineering, Boston, Massachusetts, United States of America\n",
      "\n",
      "# Abstract\n",
      "\n",
      "# Introduction\n",
      "\n",
      "Understanding the temporal patterns in disease occurrence is valuable for formulating effective disease preventive progr\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[0].page_content[:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8bbe93d7-6ab9-43d8-bf87-1a6016f19cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy Embeddings -- a bit faster than the nomic ones\n",
    "# we save the embeddings into a in-memory vector store from sklearn, that is SKLearnVectorStore\n",
    "persist_path = f\"/projects/0/prjs1194/embeddings/\"+\"union.bson\"\n",
    "\n",
    "def create_embeddings(out_path, docs):\n",
    "    folder = os.path.dirname(out_path)\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    \n",
    "    if exists(out_path):\n",
    "        vectorstore = SKLearnVectorStore(\n",
    "            embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    "            persist_path=out_path,\n",
    "            serializer=\"bson\") \n",
    "        print(\"Vector store was loaded from\", out_path)\n",
    "        \n",
    "    else:\n",
    "        vectorstore = SKLearnVectorStore.from_texts(\n",
    "            texts=[doc.page_content for doc in docs],\n",
    "            embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    "            persist_path=out_path,\n",
    "            serializer=\"bson\",\n",
    "            metadatas=[doc.metadata for doc in docs])\n",
    "        vectorstore.persist()\n",
    "        print(\"Vector store was persisted to\", out_path)\n",
    "        \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb6f47a3-7379-4d1d-98ed-9ac7284f866a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store was loaded from /projects/0/prjs1194/embeddings/union.bson\n"
     ]
    }
   ],
   "source": [
    "vectorstore = create_embeddings(persist_path, docs=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7aee1b2c-7bc2-4132-8c2a-f14d22711f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bfe7df-e551-4415-8305-dcfe386163a3",
   "metadata": {},
   "source": [
    "You can check if the retriever works by asking to retrieve documents similar to your search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92fa4aff-ec64-4bac-8f91-ef37a5ccbf84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enviroment_docs = retriever.invoke(\"dracunculus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d524ed9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'fbaa1337-ec11-4a9a-b3ed-a05eaa2d821c',\n",
       " 'doi': 'paper 10.1016/j.envres.2009.02.008 36'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enviroment_docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "057ef683-9a26-4c61-82be-84a25f9553a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Griffiths, J.K., 1998. Human cryptosporidiosis: epidemiology, transmission, clinical disease, treatment, and diagnosis. Adv. Parasitol. 40, 37â€“85.\\n\\nHarvell, C.D., Mitchell, C.E., Ward, J.R., Altizer, '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enviroment_docs[0].page_content[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f176c2-362c-4f39-ba29-1e1bc72520bd",
   "metadata": {},
   "source": [
    "# build a langGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecad0635-9c43-478f-a77b-06222a91d67b",
   "metadata": {},
   "source": [
    "set up a state. A state is a schema that is going to survive across the steps above. All inputs/output across the prompting is going to survive across the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "981b57cd-f30e-47ed-94fc-9e0c46197643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the class state \n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Graph state is a dictionary that contains information we want to propagate to, and modify in, each graph node.\n",
    "    \"\"\"\n",
    "    question : str # User question\n",
    "    generation : str # LLM generation\n",
    "    max_retries : int # Max number of retries for answer generation \n",
    "    answers : int # Number of answers generated\n",
    "    loop_step: Annotated[int, operator.add] \n",
    "    documents : List[str] # List of retrieved documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2a9696-61ea-499d-b8cc-8e0c15f01c35",
   "metadata": {},
   "source": [
    "Take each of the steps above, and wrapping those into individual functions (which are nodes). They take state as input, and modify it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ba1a1-e2ea-45a8-97b2-f435dba67b38",
   "metadata": {},
   "source": [
    "Some of the functions are 'edges', so they basically evaluate the input and decide where to go (which nodes) next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db20b7a2-d13f-4d51-b8c4-0f45a53bb61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "### Nodes\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Write retrieved documents to documents key in state\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents}\n",
    "    \n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    loop_step = state.get(\"loop_step\", 0)\n",
    "    \n",
    "    rag_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "\n",
    "    Here is the context to use to answer the question:\n",
    "\n",
    "    {context} \n",
    "\n",
    "    Think carefully about the above context. \n",
    "\n",
    "    Now, review the user question:\n",
    "\n",
    "    {question}\n",
    "\n",
    "    Provide an answer to this questions using only the above context. \n",
    "\n",
    "    Use 10 sentences maximum and keep the answer concise.\n",
    "\n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    \n",
    "    # RAG generation\n",
    "    docs_txt = format_docs(documents)\n",
    "    rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    return {\"generation\": generation, \"loop_step\": loop_step+1}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag AND THAT'S IT\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}. \n",
    "\n",
    "    This carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n",
    "\n",
    "    Return JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question.\"\"\"\n",
    "    \n",
    "    doc_grader_instructions = \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "\n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\"\"\"\n",
    "    \n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\" \n",
    "    for d in documents:\n",
    "        doc_grader_prompt_formatted = doc_grader_prompt.format(document=d.page_content, question=question)\n",
    "        result = llm_json_mode.invoke([SystemMessage(content=doc_grader_instructions)] + [HumanMessage(content=doc_grader_prompt_formatted)])\n",
    "        grade = json.loads(result.content)['binary_score']\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"web_search\": web_search}\n",
    "    \n",
    "\n",
    "\n",
    "### Edges\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    max_retries = state.get(\"max_retries\", 3) # Default to 3 if not provided\n",
    "\n",
    "    hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(documents=format_docs(documents), generation=generation.content)\n",
    "    result = llm_json_mode.invoke([SystemMessage(content=hallucination_grader_instructions)] + [HumanMessage(content=hallucination_grader_prompt_formatted)])\n",
    "    grade = json.loads(result.content)['binary_score']\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        # Test using question and generation from above \n",
    "        answer_grader_prompt_formatted = answer_grader_prompt.format(question=question, generation=generation.content)\n",
    "        result = llm_json_mode.invoke([SystemMessage(content=answer_grader_instructions)] + [HumanMessage(content=answer_grader_prompt_formatted)])\n",
    "        grade = json.loads(result.content)['binary_score']\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        elif state[\"loop_step\"] <= max_retries:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: MAX RETRIES REACHED---\")\n",
    "            return \"max retries\"  \n",
    "    elif state[\"loop_step\"] <= max_retries:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\"\n",
    "    else:\n",
    "        print(\"---DECISION: MAX RETRIES REACHED---\")\n",
    "        return \"max retries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34dc5277-9027-4388-a4af-9dbc200cca68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the StateGraph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generate\n",
    "\n",
    "# Build graph\n",
    "# Directly set \"retrieve\" as the entry point, skipping route_question and web_search\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "\n",
    "# Set edges to transition between nodes\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")  # After retrieving, grade documents\n",
    "workflow.add_edge(\"grade_documents\", \"generate\")  # After grading, generate the response\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "997d67a0-7ec4-4e5e-8029-ef4d63b2b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def print_doi(doi):\n",
    "    doi_pattern = r\"10.\\d{4,9}/[-._;()/:A-Za-z0-9]+\"\n",
    "    doi_match = re.search(doi_pattern, doi)\n",
    "    \n",
    "    if doi_match:\n",
    "        return doi_match.group(0)\n",
    "    return print(\"no valid DOI has been found\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3531ed5a-d6c6-41ea-a0a0-53775306b164",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m question \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhich pathogens are mentioned in the texts?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      2\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_retries\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m}\n\u001b[1;32m      4\u001b[0m documents_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mstream(question, stream_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m event \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m], AIMessage):\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# Print only the content of the AIMessage\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28mprint\u001b[39m(event[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1315\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1310\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1311\u001b[0m         interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1312\u001b[0m         interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1313\u001b[0m         manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1314\u001b[0m     ):\n\u001b[0;32m-> 1315\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1316\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1317\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1318\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1319\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1320\u001b[0m         ):\n\u001b[1;32m   1321\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langgraph/pregel/runner.py:56\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m     54\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langgraph/pregel/retry.py:29\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     27\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langgraph/utils/runnable.py:410\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langgraph/utils/runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[42], line 95\u001b[0m, in \u001b[0;36mgrade_documents\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[1;32m     94\u001b[0m     doc_grader_prompt_formatted \u001b[38;5;241m=\u001b[39m doc_grader_prompt\u001b[38;5;241m.\u001b[39mformat(document\u001b[38;5;241m=\u001b[39md\u001b[38;5;241m.\u001b[39mpage_content, question\u001b[38;5;241m=\u001b[39mquestion)\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mllm_json_mode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_grader_instructions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_grader_prompt_formatted\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     grade \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(result\u001b[38;5;241m.\u001b[39mcontent)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# Document relevant\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain_ollama/chat_models.py:644\u001b[0m, in \u001b[0;36mChatOllama._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate\u001b[39m(\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    639\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    643\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatResult:\n\u001b[0;32m--> 644\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    647\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m final_chunk\u001b[38;5;241m.\u001b[39mgeneration_info\n\u001b[1;32m    648\u001b[0m     chat_generation \u001b[38;5;241m=\u001b[39m ChatGeneration(\n\u001b[1;32m    649\u001b[0m         message\u001b[38;5;241m=\u001b[39mAIMessage(\n\u001b[1;32m    650\u001b[0m             content\u001b[38;5;241m=\u001b[39mfinal_chunk\u001b[38;5;241m.\u001b[39mtext,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m    655\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain_ollama/chat_models.py:545\u001b[0m, in \u001b[0;36mChatOllama._chat_stream_with_aggregation\u001b[0;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chat_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatGenerationChunk:\n\u001b[1;32m    544\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 545\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_stream(messages, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    547\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m ChatGenerationChunk(\n\u001b[1;32m    548\u001b[0m                 message\u001b[38;5;241m=\u001b[39mAIMessageChunk(\n\u001b[1;32m    549\u001b[0m                     content\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m                 ),\n\u001b[1;32m    563\u001b[0m             )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/langchain_ollama/chat_models.py:527\u001b[0m, in \u001b[0;36mChatOllama._create_chat_stream\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m    518\u001b[0m         model\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    519\u001b[0m         messages\u001b[38;5;241m=\u001b[39mollama_messages,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m         tools\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mchat(\n\u001b[1;32m    528\u001b[0m         model\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    529\u001b[0m         messages\u001b[38;5;241m=\u001b[39mollama_messages,\n\u001b[1;32m    530\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    531\u001b[0m         options\u001b[38;5;241m=\u001b[39mOptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m    532\u001b[0m         keep_alive\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_alive\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    534\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/ollama/_client.py:87\u001b[0m, in \u001b[0;36mClient._stream\u001b[0;34m(self, method, url, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m   e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     85\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_lines():\n\u001b[1;32m     88\u001b[0m   partial \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n\u001b[1;32m     89\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m e \u001b[38;5;241m:=\u001b[39m partial\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpx/_models.py:863\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    861\u001b[0m decoder \u001b[38;5;241m=\u001b[39m LineDecoder()\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_text():\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m decoder\u001b[38;5;241m.\u001b[39mdecode(text):\n\u001b[1;32m    865\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m line\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpx/_models.py:850\u001b[0m, in \u001b[0;36mResponse.iter_text\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    848\u001b[0m chunker \u001b[38;5;241m=\u001b[39m TextChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 850\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m byte_content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_bytes():\n\u001b[1;32m    851\u001b[0m         text_content \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(byte_content)\n\u001b[1;32m    852\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(text_content):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpx/_models.py:831\u001b[0m, in \u001b[0;36mResponse.iter_bytes\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    829\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 831\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_raw():\n\u001b[1;32m    832\u001b[0m         decoded \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(raw_bytes)\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(decoded):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpx/_models.py:885\u001b[0m, in \u001b[0;36mResponse.iter_raw\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    882\u001b[0m chunker \u001b[38;5;241m=\u001b[39m ByteChunker(chunk_size\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request):\n\u001b[0;32m--> 885\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m raw_stream_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream:\n\u001b[1;32m    886\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes_downloaded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(raw_stream_bytes)\n\u001b[1;32m    887\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker\u001b[38;5;241m.\u001b[39mdecode(raw_stream_bytes):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpx/_client.py:127\u001b[0m, in \u001b[0;36mBoundSyncStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpx/_transports/default.py:116\u001b[0m, in \u001b[0;36mResponseStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m typing\u001b[38;5;241m.\u001b[39mIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpcore_stream:\n\u001b[1;32m    117\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:367\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:363\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream:\n\u001b[1;32m    364\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_sync/http11.py:349\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 349\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_sync/http11.py:341\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_body\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request, kwargs):\n\u001b[0;32m--> 341\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39m_receive_response_body(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    342\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_sync/http11.py:210\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_body\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    207\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mData):\n\u001b[1;32m    212\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "question = {\"question\": \"Which pathogens are mentioned in the texts?\", \n",
    "            \"max_retries\": 1}\n",
    "\n",
    "documents_list = []\n",
    "\n",
    "for event in graph.stream(question, stream_mode=\"values\"):\n",
    "    if \"generation\" in event and isinstance(event[\"generation\"], AIMessage):\n",
    "        # Print only the content of the AIMessage\n",
    "        print(event[\"generation\"].content)\n",
    "\n",
    "        # Get the unique references\n",
    "        references = []\n",
    "        print('================================ References ================================')\n",
    "        for ref in event['documents']:\n",
    "            if ref.metadata['doi'] not in references:\n",
    "                doi = print_doi(ref.metadata['doi'])\n",
    "                references.append(doi)\n",
    "                print(f\"DOI: {doi}\")\n",
    "    \n",
    "    # event contains documents (which are stored under the key 'documents')\n",
    "    if \"documents\" in event:\n",
    "        # append the documents to the documents_list, but do not print them\n",
    "        documents_list.extend(event[\"documents\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mine-dd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
